{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNGS Dataset Analysis\n",
    "Here, we will detail a basic framework for generating derivatives with the FNGS pipeline for all of your subjects in your dataset at once. For the purposes of this tutorial, we assume that each subject has 1 anatomical and 1 resting state scan per scan session.\n",
    "\n",
    "## Subject List Maintenance\n",
    "To begin, we collect a list of all the resting state nifti files, and save them to a textfile. In your terminal window (note that we are not in python yet) call something like the following, and save it to a textfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ../tests/data/ -maxdepth 1 -mindepth 1 -name \"*fMRI*.nii*\" > testrest.txt\n",
    "cat testrest.txt\n",
    "find ../tests/data/ -maxdepth 1 -mindepth 1 -name \"*MPRAGE*.nii*\" > testanat.txt\n",
    "cat testanat.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we give terminal the directory where we expect some rest files to be using the find command, specify exactly how many directory levels we expect our input to go (expect to be same level as the directory we are looking in here, so we specify 1), and then finally specify some keywords that would be present in our fMRI filenames that would not be found in other filenames. We repeat this for our structural scans, and note that we need the subjects to match up row-wise in our specification files (since we want the functional images of one subject to be analyzed with a structural scan of that same subject).\n",
    "\n",
    "#### Expected Alternate Dataset Organization Methods\n",
    "\n",
    "While processing brain graphs, you will probably come across several common dataset organization hierarchies. Here, we will detail two common ones (from the CoRR dataset) and show you how we might handle them. If our data was organized a bit differently, such as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    +-- /BNU_1/ # directory where we have all our subjects  \n",
    "    |    +-- 0025864/ # subject directory  \n",
    "    |        +-- session_1/  \n",
    "    |             +-- rest/BNU1_0025864_1_rest.nii.gz   \n",
    "    |             +-- anat/BNU1_0025864_1_anat.nii.gz  \n",
    "    |        +-- session_2/  \n",
    "    |             +-- rest/BNU1_0025864_2_rest.nii.gz \n",
    "    |             +-- anat/BNU1_0025864_2_anat.nii.gz\n",
    "    |    +-- 0021002/ and so on for all subjects...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this potential dataset organization, we can very easily replicate the above procedure, with a few small changes, and due to the fact that we have an anatomical scan at each level we have a rest scan, the call will again be very simple. We might make a call like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ./BNU_1/ -maxdepth 4 -mindepth 4 -name \"*rest*.nii.gz\" > bnurest.txt\n",
    "find ./BNU_1/ -maxdepth 4 -mindepth 4 -name \"*anat*.nii.gz\" > bnuanat.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    +-- /NKI/ # directory where we have all our subjects  \n",
    "    |    +-- 0021001/ # subject directory  \n",
    "    |        +-- session_1/  \n",
    "    |             +-- rest_645/NKI_0021001_1_rest.nii.gz\n",
    "    |             +-- rest_1400/NKI_0021001_1_rest.nii.gz\n",
    "    |             +-- anat/NKI_0021001_anat.nii.gz  \n",
    "    |        +-- session_2/  \n",
    "    |             +-- rest_645/NKI_0021001_2_rest.nii.gz\n",
    "    |             +-- rest_1400/NKI_0021001_2_rest.nii.gz\n",
    "    |    +-- 0021002/ and so on for all subjects...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might need a more complex command to obtain these files. Note that here, we only have one anatomical scan per pair of functional scan sessions. This is because the anatomical scan is only imaging the structural properties of the brain; in a typical fMRI study, where each session is conducted in a small time window, we would not expect any significant structural changes for healthy patients, so often experimenters will only collect one anatomical scan for each subject, regardless of the number of scanning sessions. If your data is organized such as this, you might need a bit more complex of a function call to get the functional file specification and anatomical file specifications to match up properly. We might make a call something like this, first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ./NKI/ -maxdepth 4 -mindepth 4 -wholename \"*rest_645*rest.nii.gz\" > nkirest.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the 'wholename' option with the find command, instead of 'name' (wholename allows us to exclude directories, and not just in the filename itself; here we want to exclude any subject without a TR of 645 so we specify the folder TR=645 would be placed into in our find command). Then, to get the anatomical files organized properly, we first take a look at our directory structure, and then can make substitutions in our resting file to adjust in the anatomical paths as well (this will lead to a perfect matching of subjects, since you are only substituting paths within the subject's portion of the directory structure and not substituting between subjects). I have found this way simplest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp nkirest.txt nkianat.txt\n",
    "# begin by in-place substituting the session where the anatomical\n",
    "# scan is found; in this case, the anatomical scans for session_1\n",
    "# are in the session_1 directory, and the anatomical scans for\n",
    "# session_2 are in the session_1 directory.\n",
    "sed -i 's/session_2/session_1/g' nkianat.txt\n",
    "# then, substitute any keywords that might be different between rest\n",
    "# scans and anatomical scans\n",
    "sed -i 's/rest_645/anat/g' nkianat.txt # replace the directory name\n",
    "sed -i 's/1_rest/anat/g' nkianat.txt # replace the filename for sess_1\n",
    "sed -i 's/2_rest/anat/g' nkianat.txt # replace the filename for sess_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few of the potential directory structures you might come across, but just about every dataset I have analyzed is in one of these two structures (or a similar structure), so hopefully you won't require too much manipulation to organize your structural and functional specification files. If you have any questions, feel free to make an issue, or shoot me an email at ericwb95@gmail.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multigraph Processing\n",
    "\n",
    "### Python Scripts\n",
    "Now that we have our two specification files, we are ready to do some multigraph processing. Here, we will open up the 'fngs_multigraph.py' script found in this directory, and explain what's going on, so let's begin by loading some dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from ndmg.scripts.fngs_pipeline import fngs_pipeline\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too out of the ordinary here; we import in our pipeline and a multiprocessing module, which allows us to spawn processes for each subject and terminates all of the memory being used by a particular subject upon completion of the subject. This is useful because several packages used for quality control (ie, matplotlib) do not effectively clear their cache in between runs, so having a separate process for each subject that can be terminated in its entirity upon completion eliminates this cache problem. We add our dependencies that will be consistent between subjects:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atlas = \"/path/to/atlas.nii.gz\"\n",
    "atlas_brain = \"/path/to/atlas/brain.nii.gz\n",
    "mask = \"/path/to/atlas/mask.nii.gz\"\n",
    "labels = [\"/path/to/label/in/atlas/brainspace.nii.gz\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that we want to use the same atlas, atlas brain, mask, and labelled atlases for every subject (as detailed in the single subject tutorial, we want all subjects to be in the same brain space to make accurate downstream inferences from our timeseries). We then specify our resting and anatomical files, which were computed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restfile = 'bnurest.txt'\n",
    "anatfile = 'bnuanat.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And are then ready to spawn our processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(results.restfile) as restfile:\n",
    "    with open(results.anatfile) as anatfile:\n",
    "        restpaths = [l[:-1] for l in restfile.readlines()]\n",
    "        anatpaths = [l[:-1] for l in anatfile.readlines()]\n",
    "        for (rest, anat) in zip(restpaths, anatpaths):\n",
    "            try:\n",
    "                p = Process(target=fmri_pipeline, args=(rest, anat,\n",
    "                            atlas, atlas_brain, mask, labels,\n",
    "                            outdir),\n",
    "                            kwargs={'clean':False, 'fmt':'graphml'})\n",
    "                p.start()\n",
    "                p.join()\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running from Bash\n",
    "As we provide entry points for the fngs pipeline, the pipeline can alternatively be run for multigraph processing from a shell script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# fngs_multigraph.sh\n",
    "#$1 = restfile\n",
    "#$2 = anatfile\n",
    "#$3 = /path/to/outdir\n",
    "\n",
    "atlas='/path/to/atlas.nii.gz'\n",
    "atlas_brain='/path/to/atlas/brain.nii.gz'\n",
    "atlas_mask='/path/to/atlas/mask.nii.gz'\n",
    "label='/path/to/labels.nii.gz'\n",
    "\n",
    "exec 4<$1\n",
    "exec 5<$2\n",
    "\n",
    "while read -r rest <&4 && read -r anat<&5; do\n",
    "    fngs_pipeline $rest $anat $atlas $atlas_brain $atlas_mask $3 $label -fmt graphml\n",
    "done 4<$1 5<$2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can execute this from command line using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash fngs_multigraph.sh restfiles.txt anatfiles.txt /path/to/outputdir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
